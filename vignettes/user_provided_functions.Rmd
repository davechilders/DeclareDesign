---
title: "How to Design and Pre-Register Experimental Designs with Baseline Data"
author: "Experiments in Governance and Politics (EGAP)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{How to Pre-Register a Simple Experimental Design using the registration package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
This vignette demonstrates how to design and pre-register an experiment after baseline data has been collected.

```{r, echo = FALSE}
library(experimentr)
library(knitr)
set.seed(20)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = TRUE, message = FALSE, 
                      eval = TRUE, tidy = TRUE, tidy.opts = list(width.cutoff = 50),
                      strip_white = TRUE, results = 'asis', fix.ext = 'pdf')
```

# A simple experiment

## Designing the experiment

First, we define the sample frame from which we will draw a sample to run the experiment from. This includes defining the levels of analysis and the covariates at each level. The below example shows how to construct a variable with a user-defined function, here sampling income levels from 1 to 25 with replacement.

```{r}
make_clustered_data <- function(J = 10, n = 100, treatment_effect = .25, ICC = .1){
  ## Inspired by Mathieu et al, 2012, Journal of Applied Psychology
  if (J %% 2 != 0 | n %% 2 !=0) {
    stop(paste("Number of clusters (J) and size of clusters (n) must be even."))
  }
  Y0_j         <- rnorm(J,0,sd = (1 + treatment_effect) ^ 2 * sqrt(ICC))
  fake_data    <- expand.grid(i = 1:n,j = 1:J)
  fake_data$Y0 <- rnorm(n * J,0,sd = (1 + treatment_effect) ^ 2 * sqrt(1 - ICC)) + Y0_j[fake_data$j]
  fake_data$Y1 <- with(fake_data,mean(Y0) + treatment_effect + (Y0 - mean(Y0)) * (2 / 3))
  fake_data$Z  <- ifelse(fake_data$j %in% sample(1:J,J / 2) == TRUE, 1, 0)
  fake_data$Y  <- with(fake_data, Z * Y1 + (1 - Z) * Y0)
  return(fake_data)
}

make_clustered_data_experimentr <- function(N_per_level = NULL){
  if(is.null(N_per_level))
    if(N_per_level[1] %% N_per_level[2] > 0)
      stop("You can only use a design with equally sized clusters.")
  if(length(N_per_level)!=2)
    stop("You can only use this custom DGP function if there are exactly two levels.")
  return(make_clustered_data(J = N_per_level[2], n = N_per_level[1]/N_per_level[2]))
}


population <- declare_population(custom_population_function = make_clustered_data_experimentr, 
                             N_per_level = c(1000, 10))
```

Second, we define the potential outcomes, which will be simulated based on the baseline covariate data. Here we illustrate a custom function that determines potential outcomes -- a binomial distribution with a probit link. You can type any function, including one you make up, in place of rbinom. (Or none at all for OLS.)

```{r}
potential_outcomes     <-  declare_potential_outcomes(
  condition_names = c("Z0","Z1"),
  outcome_formula = Y_star ~ rbinom(500, size = 1, prob = pnorm(Y0*Z0 + Y1*Z1))
)
```

Fourth, we define one or more analyses we will run based on simulated data. This analysis will also be used for power analysis. Here we define a custom estimator (probit regression) and a custom QOI function (for this example, it just pulls the probit coefficient, which is note an estimate of any causal quantity).

```{r}
probit_regression <- function(formula, subset = NULL, weights = NULL, data, ...){
  args_list <- c(list(formula = formula, subset = subset, weights = weights, data = data), list(...))
  args_list$family <- binomial(link = "probit")
  do.call(glm, args = args_list)
}

probit_qoi_temp <- function(fit, statistics = c("est", "se", "p", "ci_lower", "ci_upper")){
  coef <- summary(fit)$coefficients[2, 1]
  se <- summary(fit)$coefficients[2, 2]
  p <- summary(fit)$coefficients[2, 4]
  ci_upper <- coef + 1.96*se
  ci_lower <- coef - 1.96*se
  stats <- matrix(c(coef, se, p, ci_lower, ci_upper), dimnames = list(c("est", "se", "p", "ci_lower", "ci_upper"), "Y~Zlm"))
  return(stats)
}

analysis      <- declare_analysis(formula = Y_star ~ Z_star, treatment_variable = "Z_star", 
                                  estimator = probit_regression, quantity_of_interest = probit_qoi_temp)
```

Then we declare the design of the experiment, in this case a simple one without clusters or blocking. Here we demonstrate how to use a custom random assignment function, in this case simple random sampling of "Z0" and "Z1".

```{r}
random_clusters <- function(data){
  return(sample(1:10, nrow(data), replace = T))
}
```

```{r}
random_blocks <- function(data){
  return(data$random_clusters < 10)
}
```

```{r}
assign_function <- function()
    return(sample(c("Z0", "Z1"), 500, replace = T))
  
design_custom <- declare_design(potential_outcomes = potential_outcomes, 
                                custom_assignment_function = assign_function, 
                                treatment_variable = "Z_star",
                                block_variable_name = "random_block",
                                custom_block_function = random_blocks,
                                cluster_variable_name = "random_cluster",
                                custom_cluster_function = random_clusters)
```

Before finalizing the design, we conduct a power analysis to determine whether 500 units and 10 clusters (villages) are sufficient. To do this, we use the diagnose function.

```{r}
simulations <- diagnose(population = population, 
                        design = design, 
                        analysis = analysis)
kable(summary(simulations), digits = 3)
```

## Mock analysis

After settling on a sample size and a final design, we can conduct a mock analysis of the data to ensure we are satisfied with the analysis of the data. To do this, we create mock data -- simulated from the distributions we set -- and then run the analyses on the simulated data.

```{r}
mock <- draw_population(population = population)
mock <- reveal_design(data = mock, design = design)
kable(head(mock), digits = 3)
```

```{r}
kable(get_estimates(analysis = analysis, data = mock))
```
